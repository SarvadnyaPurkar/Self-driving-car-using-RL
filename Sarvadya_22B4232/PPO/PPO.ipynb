{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f2f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.actor = nn.Linear(256, action_dim)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        value = self.critic(x)\n",
    "        return mean, std, value\n",
    "\n",
    "def run_ppo(episodes, is_training, render=False):\n",
    "    env = gym.make('Pendulum-v1', render_mode='human' if render else None)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = float(env.action_space.high[0])\n",
    "    \n",
    "    # Hyperparameters\n",
    "    gamma = 0.99\n",
    "    lr = 3e-4\n",
    "    epsilon = 0.2\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    max_steps = 200\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ActorCritic(state_dim, action_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    if not is_training:\n",
    "        model.load_state_dict(torch.load('ppo_pendulum.pth'))\n",
    "        model.eval()\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards_ep = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        dones = []\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Collect trajectory\n",
    "        for _ in range(max_steps):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            with torch.set_grad_enabled(is_training):\n",
    "                mean, std, value = model(state_tensor)\n",
    "                dist = Normal(mean, std)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action).sum(-1)\n",
    "            \n",
    "            action_np = action.cpu().numpy().flatten()\n",
    "            action_np = np.clip(action_np, -action_bound, action_bound)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_np)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action_np)\n",
    "            rewards_ep.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "        \n",
    "        if is_training:\n",
    "            # Convert to tensors\n",
    "            states = torch.FloatTensor(np.array(states)).to(device)\n",
    "            actions = torch.FloatTensor(np.array(actions)).to(device)\n",
    "            old_log_probs = torch.cat(log_probs).detach()\n",
    "            rewards_ep = torch.FloatTensor(np.array(rewards_ep)).to(device)\n",
    "            dones = torch.FloatTensor(np.array(dones)).to(device)\n",
    "            \n",
    "            # Calculate returns\n",
    "            returns = []\n",
    "            R = 0\n",
    "            for r, done in zip(reversed(rewards_ep), reversed(dones)):\n",
    "                R = r + gamma * R * (1 - done)\n",
    "                returns.insert(0, R)\n",
    "            returns = torch.FloatTensor(returns).to(device)\n",
    "            \n",
    "            # Normalize returns\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            \n",
    "            # PPO update\n",
    "            for _ in range(epochs):\n",
    "                # Shuffle indices\n",
    "                indices = torch.randperm(len(states))\n",
    "                \n",
    "                for start in range(0, len(states), batch_size):\n",
    "                    end = start + batch_size\n",
    "                    idx = indices[start:end]\n",
    "                    \n",
    "                    # Get minibatch\n",
    "                    batch_states = states[idx]\n",
    "                    batch_actions = actions[idx]\n",
    "                    batch_old_log_probs = old_log_probs[idx]\n",
    "                    batch_returns = returns[idx]\n",
    "                    \n",
    "                    # Get current policy\n",
    "                    mean, std, values_pred = model(batch_states)\n",
    "                    dist = Normal(mean, std)\n",
    "                    log_probs = dist.log_prob(batch_actions).sum(-1)\n",
    "                    entropy = dist.entropy().mean()\n",
    "                    \n",
    "                    # Calculate ratios\n",
    "                    ratios = torch.exp(log_probs - batch_old_log_probs)\n",
    "                    \n",
    "                    # Calculate losses\n",
    "                    advantages = batch_returns - values_pred.squeeze().detach()\n",
    "                    surr1 = ratios * advantages\n",
    "                    surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "                    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                    critic_loss = F.mse_loss(values_pred.squeeze(), batch_returns)\n",
    "                    \n",
    "                    # Total loss\n",
    "                    loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "                    \n",
    "                    # Update\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                    optimizer.step()\n",
    "        \n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(rewards[-10:])\n",
    "            print(f'Episode {episode+1}/{episodes}, Reward: {ep_reward:.2f}, Avg Reward: {avg_reward:.2f}')\n",
    "            \n",
    "            if is_training:\n",
    "                torch.save(model.state_dict(), 'ppo_pendulum.pth')\n",
    "    \n",
    "    # Plot results\n",
    "    plt.plot(rewards)\n",
    "    plt.title('PPO Training - Pendulum-v1')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('ppo_pendulum.png')\n",
    "    plt.close()\n",
    "    \n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46694990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/500, Reward: -1342.64, Avg Reward: -1265.56\n",
      "Episode 20/500, Reward: -1101.37, Avg Reward: -1430.97\n",
      "Episode 30/500, Reward: -1200.13, Avg Reward: -1308.93\n",
      "Episode 40/500, Reward: -1509.21, Avg Reward: -1305.01\n",
      "Episode 50/500, Reward: -981.37, Avg Reward: -1379.17\n",
      "Episode 60/500, Reward: -880.88, Avg Reward: -1298.77\n",
      "Episode 70/500, Reward: -1555.10, Avg Reward: -1360.12\n",
      "Episode 80/500, Reward: -950.37, Avg Reward: -1163.48\n",
      "Episode 90/500, Reward: -1176.81, Avg Reward: -1172.25\n",
      "Episode 100/500, Reward: -1170.19, Avg Reward: -1366.35\n",
      "Episode 110/500, Reward: -1513.01, Avg Reward: -1264.77\n",
      "Episode 120/500, Reward: -1192.74, Avg Reward: -1293.14\n",
      "Episode 130/500, Reward: -1213.30, Avg Reward: -1328.06\n",
      "Episode 140/500, Reward: -1474.82, Avg Reward: -1351.27\n",
      "Episode 150/500, Reward: -1326.70, Avg Reward: -1356.41\n",
      "Episode 160/500, Reward: -1360.80, Avg Reward: -1389.85\n",
      "Episode 170/500, Reward: -1354.80, Avg Reward: -1355.64\n",
      "Episode 180/500, Reward: -1337.48, Avg Reward: -1432.70\n",
      "Episode 190/500, Reward: -1301.74, Avg Reward: -1265.68\n",
      "Episode 200/500, Reward: -1034.15, Avg Reward: -1453.79\n",
      "Episode 210/500, Reward: -1551.74, Avg Reward: -1206.30\n",
      "Episode 220/500, Reward: -1836.28, Avg Reward: -1267.79\n",
      "Episode 230/500, Reward: -1064.13, Avg Reward: -1198.58\n",
      "Episode 240/500, Reward: -1260.59, Avg Reward: -1196.27\n",
      "Episode 250/500, Reward: -1031.80, Avg Reward: -1150.05\n",
      "Episode 260/500, Reward: -1073.31, Avg Reward: -1297.75\n",
      "Episode 270/500, Reward: -1678.38, Avg Reward: -1322.25\n",
      "Episode 280/500, Reward: -1206.48, Avg Reward: -1274.89\n",
      "Episode 290/500, Reward: -1812.48, Avg Reward: -1223.21\n",
      "Episode 300/500, Reward: -1171.38, Avg Reward: -1420.08\n",
      "Episode 310/500, Reward: -1184.19, Avg Reward: -1065.82\n",
      "Episode 320/500, Reward: -940.02, Avg Reward: -1097.19\n",
      "Episode 330/500, Reward: -1715.00, Avg Reward: -1321.10\n",
      "Episode 340/500, Reward: -1244.87, Avg Reward: -1308.46\n",
      "Episode 350/500, Reward: -1515.81, Avg Reward: -1173.42\n",
      "Episode 360/500, Reward: -969.84, Avg Reward: -1275.11\n",
      "Episode 370/500, Reward: -1368.27, Avg Reward: -1310.18\n",
      "Episode 380/500, Reward: -1482.16, Avg Reward: -1289.41\n",
      "Episode 390/500, Reward: -1256.42, Avg Reward: -1249.30\n",
      "Episode 400/500, Reward: -1115.31, Avg Reward: -1357.15\n",
      "Episode 410/500, Reward: -1655.51, Avg Reward: -1392.17\n",
      "Episode 420/500, Reward: -1293.39, Avg Reward: -1241.94\n",
      "Episode 430/500, Reward: -1184.70, Avg Reward: -1400.94\n",
      "Episode 440/500, Reward: -1400.20, Avg Reward: -1162.72\n",
      "Episode 450/500, Reward: -1332.55, Avg Reward: -1326.71\n",
      "Episode 460/500, Reward: -1459.77, Avg Reward: -1394.88\n",
      "Episode 470/500, Reward: -1602.41, Avg Reward: -1549.34\n",
      "Episode 480/500, Reward: -1580.77, Avg Reward: -1638.63\n",
      "Episode 490/500, Reward: -1711.23, Avg Reward: -1607.52\n",
      "Episode 500/500, Reward: -1431.32, Avg Reward: -1537.67\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "run_ppo(episodes=500, is_training=True, render=False)\n",
    "\n",
    "# Testing\n",
    "run_ppo(episodes=5, is_training=False, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7313152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "run_ppo(episodes=5, is_training=False, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
